![image](https://github.com/user-attachments/assets/1b17264b-ebf6-41bc-99ef-3a415f5c636e)
# News

**2025.1** our paper has been accepted by IEEE TIP [link](https://ieeexplore.ieee.org/abstract/document/10844040?casa_token=0ULEW0Oah-sAAAAA:GLtYjOlE2D-SKq-Uct3uXeXPVTUn6zTK1fT8vw_dHyX6q8s0E0p8bKPElJrtWxUXy5thkxcnsLYyAQ).

**2024.2** [arxiv paper](https://arxiv.org/pdf/2402.18922) released.



## Dataset
![image](https://github.com/user-attachments/assets/03224d0d-9450-43ee-bde9-6de2e51c09f8)

you can find them [here](https://github.com/lartpang/awesome-segmentation-saliency-dataset#camouflaged-object-detection-cod).

## Prediction map

you can download [here](https://drive.google.com/drive/folders/1LfSkeirAlctiroPGY6rD99FsWJ0OGRfM?usp=drive_link).


## References

During development, we referred to the following resources:

- [MAE](https://github.com/facebookresearch/mae) - The backbone of SENet is built based on this project.
  Note that to get good results, you need to use the full pre-trained weights of MAE (including the decoder part). you can get the mae weight [here](https://dl.fbaipublicfiles.com/mae/visualize/mae_visualize_vit_base.pth).
- [BGNet](https://github.com/thograce/BGNet) - The training process and implementation are based on this project.

## Citation

if you think our work is helpful, please cite

```bibtex
@article{hao2025simple,
  title={A simple yet effective network based on vision transformer for camouflaged object and salient object detection},
  author={Hao, Chao and Yu, Zitong and Liu, Xin and Xu, Jun and Yue, Huanjing and Yang, Jingyu},
  journal={IEEE Transactions on Image Processing},
  year={2025},
  publisher={IEEE}
}
