import cv2
import os
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import io
import torch
from segment_anything import sam_model_registry, SamPredictor
import re
import base64
import requests
import warnings
warnings.filterwarnings("ignore")
import time


def generate_augment_imgs(image_path):
    """
    输入一张图像的路径，生成原图、水平翻转、垂直翻转和同时翻转的四张图像，
    并保存到 output 文件夹。
    """
    # 读取图像
    image = cv2.imread(image_path)
    if image is None:
        print("图像加载失败，请检查文件路径是否正确。")
        return

    output_dir = os.path.splitext(os.path.basename(image_path))[0]
    # 创建输出目录
    output_dir = f"augment/{output_dir}"
    os.makedirs(output_dir, exist_ok=True)

    img_path_list = []
    # 原图
    original_path = os.path.join(output_dir, "original.jpg")
    cv2.imwrite(original_path, image)
    img_path_list.append(original_path)

    # 水平翻转（左右镜像）
    h_flip = cv2.flip(image, 1)
    horizontal_path = os.path.join(output_dir, "horizontal_flip.jpg")
    cv2.imwrite(horizontal_path, h_flip)
    img_path_list.append(horizontal_path)

    # 垂直翻转（上下倒置）
    v_flip = cv2.flip(image, 0)
    vertical_path = os.path.join(output_dir, "vertical_flip.jpg")
    cv2.imwrite(vertical_path, v_flip)
    img_path_list.append(vertical_path)

    # 同时水平和垂直翻转（对角线翻转）
    hv_flip = cv2.flip(image, -1)
    both_path = os.path.join(output_dir, "both_flip.jpg")
    cv2.imwrite(both_path, hv_flip)
    img_path_list.append(both_path)

    print(f"✅ 四张翻转图像已成功保存至 {os.path.abspath(output_dir)}")
    return img_path_list

"""Please analyze the image and locate the {task} in it.
If present, return:
- A brief description of the object.
- Its normalized bounding box coordinates in [x_min, y_min, x_max, y_max] format, where all values are between 0 and 1. (0, 0) corresponds to the top-left corner of the image, and (1, 1) corresponds to the bottom-right corner.

Only output one object even if multiple are visible. Use the following format:

Object: <description>
BoundingBox: [<x_min>, <y_min>, <x_max>, <y_max>]"""

def build_prompt(task):
    template = """
Please analyze the image and locate the {task} in it.  
If present, return the following:

1. A brief description of the object.
2. Its normalized bounding box coordinates in the format: [x_min, y_min, x_max, y_max], where each value is between 0 and 1.

Important Coordinate Details:
- The horizontal axis is the x-axis: x=0 corresponds to the leftmost edge, and x=1 corresponds to the rightmost edge.
- The vertical axis is the y-axis: y=0 corresponds to the top edge, and y=1 corresponds to the bottom edge.
- (0, 0) refers to the top-left corner of the image, and (1, 1) refers to the bottom-right corner.

Only output one object even if multiple are visible. Use the following format:

Object: <description>
BoundingBox: [<x_min>, <y_min>, <x_max>, <y_max>]
"""
    return template.format(task=task)


def build_select_prompt(task, coords):
    prompt = f'Please analyze the image provided below and determine which of the bounding boxes better captures the {task}.\n'

    prompt += """
Your task is to:
1. Identify which bounding box more accurately includes the entire target object.
2. Provide a brief explanation for your choice.

The coordinates for each bounding box are as follows:
    """
    d = 0
    colors = ['red', 'green', 'blue', 'yellow']

    for a in coords:
        prompt += f'- **Bbox {d+1} ({colors[d]})**: {a}\n'
        d += 1
    prompt += """
Return your answer in the following format:

Best Box: <Box Number>
Reasoning: <Explanation>"""
    return prompt 

def build_optimize_prompt(task, current_box):
    prompt = f"Please analyze the image provided below and evaluate whether the current bounding box accurately captures the target object ({task}).\n"
    prompt += f"The current normalized bounding box coordinates are: {current_box}, where:\n"
    prompt += "- `x_min` = {:.2f} (left edge)\n".format(current_box[0])
    prompt += "- `y_min` = {:.2f} (top edge)\n".format(current_box[1])
    prompt += "- `x_max` = {:.2f} (right edge)\n".format(current_box[2])
    prompt += "- `y_max` = {:.2f} (bottom edge)\n".format(current_box[3])

    prompt += """
Your task is to:
1. Assess whether the current bounding box adequately includes the entire target object.
2. If the current box does not fully capture the target object or leaves unnecessary margins, suggest an optimized bounding box with improved coordinates.

Return your response in the following format:

Current Box: [x_min, y_min, x_max, y_max]
Optimized Box: [x_min_optimized, y_min_optimized, x_max_optimized, y_max_optimized]
Reasoning: <Explanation of why the optimization was made>
"""
    return prompt
# 示例调用：
# generate_augment_imgs("/home/kevin/androidworld2/SAM/image1.jpg", '1')  # 替换为你的图片路径



def extract_optimized_box(text):
    # 正则匹配 "Optimized Box: [...]" 中的内容
    pattern = r'Optimized\s+Box:\s*\[([^\]]+)\]'
    match = re.search(pattern, text)
    
    if match:
        coords_str = match.group(1)  # 获取方括号内的内容
        try:
            # 提取数字和小数点，忽略空格和逗号
            coords = [float(num) for num in re.findall(r'[-+]?\d*\.\d+|\d+', coords_str)]
            # 确保返回前四个数字
            return coords[:4]
        except Exception as e:
            print("Error parsing coordinates:", e)
            return None
    else:
        print("Could not find 'Optimized Box' in response.")
        return None

# 替换为你自己的 OpenAI API Key

def encode_image1(image_path):
    """将本地图片编码为 base64 字符串"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def call_gpt4o_with_image(image_path, prompt):
    """
    调用 GPT-4o 模型处理图像并生成回答
    :param image_path: 本地图像路径
    :param prompt: 用户对图像的提问
    :return: 模型返回的回答文本
    """
    # 图像转 base64
    api_key = "sk-q246qsn66kd7zzpnEf8001D97d404f5a9985F062B1833896"
    # api_key = 'sk-VRpcUIRxtAbnvVIn30559952BcBe4b21B5CfEf9bCfA4DcAa'
    base64_image = encode_image1(image_path)

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    payload = {
        "model": "gpt-4o",  # 注意：确保你有权限访问 gpt-4o
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 4096,
        'temperature': 0,
    }
    # print("开始等待...")
    # time.sleep(10)  # 等待 10 秒
    # print("等待结束")
    response = requests.post('https://api.gptplus5.com/v1/chat/completions',  headers=headers, json=payload, verify=False,)

    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content']
    else:
        raise Exception(f"API 请求失败: {response.status_code}, {response.text}")

# def image_to_jpeg_bytes(image: Image.Image) -> bytes:
#   in_mem_file = io.BytesIO()
#   image.save(in_mem_file, format='JPEG')
#   # Reset file pointer to start
#   in_mem_file.seek(0)
#   img_bytes = in_mem_file.read()
#   return img_bytes

# def array_to_jpeg_bytes(image: np.ndarray) -> bytes:
#   """Converts a numpy array into a byte string for a JPEG image."""
#   image = Image.fromarray(image)
#   return image_to_jpeg_bytes(image)

# def encode_image(image: np.ndarray) -> str:
#     return base64.b64encode(array_to_jpeg_bytes(image)).decode('utf-8')


# def call_gpt4o_with_image(image_path, text_prompt):
#     api_key = "sk-q246qsn66kd7zzpnEf8001D97d404f5a9985F062B1833896"
#     headers = {
#         'Content-Type': 'application/json',
#         'Authorization': f'Bearer {api_key}',
#     }

#     payload = {
#         'model': 'gpt-4o',
#         'temperature': 0.,
#         'messages': [{
#             'role': 'user',
#             'content': [
#                 {'type': 'text', 'text': text_prompt},
#             ],
#         }],
#         'max_tokens': 4096,
#     }

#     # Gpt-4v supports multiple images, just need to insert them in the content
#     # list.
#     # for image in images:
#     #   # print(123456)
#     img = Image.open(image_path)

# # 转换为 numpy 数组
#     img_array = np.array(img)
#     payload['messages'][0]['content'].append({
#           'type': 'image_url',
#           'image_url': {
#               'url': f'data:image/jpeg;base64,{encode_image(img_array)}'
#           },
#       })

#     counter = 3

#     wait_seconds = 1
#     while counter > 0:

#       try:
#         response = requests.post(
#             # 'https://api.openai.com/v1/chat/completions',
#             # 'https://az.gptplus5.com/v1/chat/completions',
#             'https://api.gptplus5.com/v1/chat/completions',
#             # 'https://api.openai.com/v1/chat/completions',
#             headers=headers,
#             json=payload,
#             verify=False
#         )

#         if response.ok and 'choices' in response.json():
#           return response.json()['choices'][0]['message']['content']
             
#         print(
#             'Error calling OpenAI API with error message: '
#             + response.json()['error']['message']
#         )
#         time.sleep(wait_seconds)
#         wait_seconds *= 2
#       except Exception as e:  # pylint: disable=broad-exception-caught
#         # Want to catch all exceptions happened during LLM calls.
#         time.sleep(wait_seconds)
#         wait_seconds *= 2
#         counter -= 1
#         print('Error calling LLM, will retry soon...')
#         print(e)
#     return None

def extract_bbox(text):
    """
    提取文本中最后连续出现的 4 个浮点数作为 bounding box。
    
    参数:
        text (str): 原始输入文本，可能包含 Object 和 BoundingBox 行。
        
    返回:
        list of float: 4个浮点数组成的 bounding box。
    """
    # 匹配所有浮点数形式的数字
    numbers = re.findall(r"[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?", text)

    # 取值部分去重提取并转换为浮点数
    numbers = [float(n[0]) for n in numbers]

    if len(numbers) >= 4:
        return numbers[-4:]  # 返回最后四个数字
    else:
        return [0.45, 0.45, 0.55, 0.55]

def extract_best_box_number(text):
    match = re.search(r'Best\s+Box:\s*(\d+)', text)
    if match:
        return match.group(1)
    else:
        return None

def get_center_point(coord):
    return [[(coord[0]+coord[2])/2, (coord[1]+coord[3])/2]]

# 使用示例
def process_coords(coords_list):
    original_coord = coords_list[0]
    horizontal_coord = coords_list[1]
    horizontal_coord = [1-horizontal_coord[2], horizontal_coord[1], 1-horizontal_coord[0], horizontal_coord[3]]
    horizontal_coord = [float(f"{x:.2f}") for x in horizontal_coord]
    vertical_coord = coords_list[2]
    vertical_coord = [vertical_coord[0], 1-vertical_coord[3], vertical_coord[2], 1-vertical_coord[1]]
    vertical_coord = [float(f"{x:.2f}") for x in vertical_coord]
    bothflip_coord = coords_list[3]
    bothflip_coord = [1-bothflip_coord[2], 1-bothflip_coord[3], 1-bothflip_coord[0], 1-bothflip_coord[1]]
    bothflip_coord = [float(f"{x:.2f}") for x in bothflip_coord]
    return [original_coord, horizontal_coord, vertical_coord, bothflip_coord]


def iou(bbox1, bbox2):
    """
    计算两个归一化 bbox 的 IoU。
    bbox: [x_min, y_min, x_max, y_max]，归一化到 [0, 1]
    """
    x1_min, y1_min, x1_max, y1_max = bbox1
    x2_min, y2_min, x2_max, y2_max = bbox2

    # 计算交集区域的坐标
    inter_x_min = max(x1_min, x2_min)
    inter_y_min = max(y1_min, y2_min)
    inter_x_max = min(x1_max, x2_max)
    inter_y_max = min(y1_max, y2_max)

    # 计算交集面积
    inter_width = max(0, inter_x_max - inter_x_min)
    inter_height = max(0, inter_y_max - inter_y_min)
    inter_area = inter_width * inter_height

    # 计算每个 bbox 的面积
    area1 = (x1_max - x1_min) * (y1_max - y1_min)
    area2 = (x2_max - x2_min) * (y2_max - y2_min)

    # 并集面积
    union_area = area1 + area2 - inter_area

    if union_area == 0:
        return 0.0

    return inter_area / union_area


def remove_duplicate_bboxes(bboxes, iou_threshold=0.75):
    """
    去除重复的 bbox。如果两个 bbox 的 IoU >= threshold，
    则保留面积较大的那个。
    """
    to_remove = set()
    n = len(bboxes)

    for i in range(n):
        for j in range(i + 1, n):
            if i in to_remove or j in to_remove:
                continue
            iou_score = iou(bboxes[i], bboxes[j])
            if iou_score >= iou_threshold:
                # 删除面积较小的那个
                area_i = (bboxes[i][2] - bboxes[i][0]) * (bboxes[i][3] - bboxes[i][1])
                area_j = (bboxes[j][2] - bboxes[j][0]) * (bboxes[j][3] - bboxes[j][1])
                if area_i > area_j:
                    to_remove.add(j)
                else:
                    to_remove.add(i)

    # 构建结果列表
    result = [bboxes[i] for i in range(n) if i not in to_remove]
    return result





def draw_bboxes_on_image(image_path, bboxes, colors=None):
    """
    在图像上绘制多个 bounding box，并按顺序编号。
    
    参数:
        image_path (str): 原始图像的路径；
        bboxes (list of list): 归一化 bbox 列表，格式为 [x_min, y_min, x_max, y_max]；
        colors (list of tuple): 每个框的颜色（RGB元组），可选；
        output_path (str): 输出图像保存路径；
        show (bool): 是否显示图像，默认显示。
    """
    # 默认颜色列表
    if colors is None:
        colors = [
            (255, 0, 0),     # 红色
            (0, 255, 0),     # 绿色
            (0, 0, 255),     # 蓝色
            (255, 255, 0),   # 黄色
            (255, 0, 255),   # 品红
            (0, 255, 255)    # 青色
        ]

    # 加载图像
    image = Image.open(image_path).convert("RGB")
    draw = ImageDraw.Draw(image)
    width, height = image.size

    # 字体设置
    try:
        font = ImageFont.truetype("arial.ttf", 20)
    except:
        font = ImageFont.load_default()

    # 绘制每个 bbox
    for idx, bbox in enumerate(bboxes):
        x_min, y_min, x_max, y_max = bbox

        # 归一化坐标转像素坐标
        left = x_min * width
        top = y_min * height
        right = x_max * width
        bottom = y_max * height

        color = colors[idx % len(colors)]

        # 绘制矩形框
        draw.rectangle([left, top, right, bottom], outline=color, width=3)

        # 添加编号文本
        text = str(idx + 1)
        # text_size = draw.textsize(text, font=font)
        text_bbox = draw.textbbox((0, 0), text, font=font)
        text_size = (text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1])
        text_position = (int(left) + 5, int(top) - text_size[1] if top > text_size[1] else int(top))

        # 文字背景框
        draw.rectangle(
            [text_position[0], text_position[1],
             text_position[0] + text_size[0],
             text_position[1] + text_size[1]],
            fill=color
        )
        draw.text(text_position, text, fill="white", font=font)

    # 保存图像
    output_dir = os.path.splitext(os.path.basename(image_path))[0]
    # 创建输出目录
    output_dir = f"augment/{output_dir}"
    output_path = output_dir + '/with_multi_boxes.jpg'
    image.save(output_path)
    print(f"图像已保存至: {output_path}")

    # 显示图像

    return output_path

def draw_bbox_on_image(image_path, normalized_bbox_coords, image_name):
    """
    在图片上绘制边界框并保存新图片。
    
    :param image_path: 输入图片的路径
    :param normalized_bbox_coords: 归一化后的边界框坐标 (x1, y1, x2, y2)，范围在 [0, 1]
    :param output_path: 输出图片的路径
    """
    # 打开图片
    img = Image.open(image_path)
    
    # 获取图片的宽度和高度
    width, height = img.size
    
    # 将归一化坐标转换为像素坐标
    x1, y1, x2, y2 = normalized_bbox_coords
    pixel_x1 = int(x1 * width)
    pixel_y1 = int(y1 * height)
    pixel_x2 = int(x2 * width)
    pixel_y2 = int(y2 * height)
    
    # 创建绘图对象
    draw = ImageDraw.Draw(img)
    
    # 绘制红色边界框
    draw.rectangle([pixel_x1, pixel_y1, pixel_x2, pixel_y2], outline="red", width=3)  # 宽度为 3 的红色边框
    output_dir = os.path.splitext(os.path.basename(image_path))[0]
    # 创建输出目录
    output_dir = f"augment/{output_dir}"
    output_path = output_dir + f'/{image_name}.jpg'

    # 保存新图片
    img.save(output_path)
    print(f"边界框已绘制并保存到 {output_path}")
    return output_path

def load_sam_model(model_type="vit_b", checkpoint_path="sam_check_point/sam_vit_b_01ec64.pth"):
    """
    加载 SAM 模型（只需调用一次）
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Loading SAM model...")
    sam = sam_model_registry[model_type](checkpoint=checkpoint_path)
    sam.to(device=device)
    predictor = SamPredictor(sam)
    return predictor

def absolute_to_relative(bbox, image_width, image_height):
    """
    将 BBox 绝对坐标 (x_min, y_min, x_max, y_max) 转换为相对坐标
    """
    x_min, y_min, x_max, y_max = bbox
    return [
        x_min / image_width,
        y_min / image_height,
        x_max / image_width,
        y_max / image_height
    ]

def relative_to_absolute(bbox, image_width, image_height):
    """
    将 BBox 相对坐标 (x_min, y_min, x_max, y_max) 转换为绝对坐标
    """
    x_min, y_min, x_max, y_max = bbox
    return [
        int(round(x_min * image_width)),
        int(round(y_min * image_height)),
        int(round(x_max * image_width)),
        int(round(y_max * image_height))
    ]
    
def run_sam_segmentation(
    image_path,
    point_coords=None,
    point_labels=None,
    bbox=None,
    model_type="vit_b",
    checkpoint_path="sam_check_point/sam_vit_b_01ec64.pth",
    save_overlay=True,
    save_mask=True
):
    """
    使用 SAM 进行图像分割，支持点提示 + BBox 提示

    参数：
        image_path (str): 输入图像路径
        point_coords (np.ndarray or list): 点坐标的列表 [[x1, y1], [x2, y2], ...]
        point_labels (np.ndarray or list): 点的标签（1: 正样本，0: 负样本）
        bbox (np.ndarray or list): BBox 坐标 [x_min, y_min, x_max, y_max]
        model_type (str): SAM 的模型类型，如 vit_b / vit_l / vit_h
        checkpoint_path (str): 模型权重路径
        output_dir (str): 输出目录
        save_overlay (bool): 是否保存叠加图
        save_mask (bool): 是否保存二值 mask 图
    返回：
        best_mask (np.ndarray): 二值 mask 图像数组
        overlayed_image (np.ndarray): 叠加后的图像
    """
    # 加载模型（只加载一次）
    predictor = load_sam_model(model_type, checkpoint_path)

    # 读取图像并转换为 RGB 格式
    image = cv2.imread(image_path)
    h, w, c = image.shape
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # 设置图像嵌入（只需一次）
    predictor.set_image(image_rgb)

    # 处理输入提示
    point_coords = [[point_coords[0][0]*w, point_coords[0][1]*h]]
    point_coords = np.array(point_coords) if point_coords is not None else None
    point_labels = np.array(point_labels) if point_labels is not None else None

    bbox = [bbox[0]*w, bbox[1]*h, bbox[2]*w, bbox[3]*h]
    bbox = np.array(bbox) if bbox is not None else None

    # 执行预测
    masks, scores, logits = predictor.predict(
        point_coords=point_coords,
        point_labels=point_labels,
        box=bbox,
        multimask_output=True
    )

    # 选择置信度最高的 mask
    best_mask = masks[0]  # 第一个 mask 置信度最高

    # 创建输出目录
    output_dir = os.path.splitext(os.path.basename(image_path))[0]
    # 创建输出目录
    output_dir = f"output/{output_dir}"
    os.makedirs(output_dir, exist_ok=True)

    overlayed_image = None
    if save_mask:
        # 保存原始二值 mask
        mask_output_path = os.path.join(output_dir, "sam_mask_binary.png")
        mask_image = best_mask * 255
        mask_image = mask_image.astype(np.uint8)
        cv2.imwrite(mask_output_path, mask_image)
        print(f"原始二值 mask 已保存至: {mask_output_path}")

    if save_overlay:
        # 创建彩色 mask（红色）
        color = np.array([255, 0, 0])
        h, w = best_mask.shape
        color_mask = np.zeros((h, w, 3), dtype=np.uint8)
        color_mask[best_mask] = color

        # 叠加 mask 到原图上（半透明方式）
        overlayed_image = cv2.addWeighted(image, 1.0, color_mask, 0.5, 0)

        # 保存叠加图像
        overlay_output_path = os.path.join(output_dir, "sam_mask_overlayed.png")
        cv2.imwrite(overlay_output_path, overlayed_image)
        print(f"叠加 mask 的图像已保存至: {overlay_output_path}")

    return best_mask, overlayed_image

# 示例调用
if __name__ == "__main__":
    # image_path = "/home/kevin/androidworld2/SAM/images/dog2.JPG"
    # user_prompt = "请描述这张图片的内容。"
    # result = call_gpt4o_with_image(image_path, user_prompt)
    # print(result)
    # text = ' 0.4 0.5 0.6 0.7 0.9 40 [10, 20.1] 0.7888'
    # print(extract_bbox(text))
    # coords = [[0.1, 0.2, 0.3, 0.4],[0.1, 0.2, 0.3, 0.4],[0.1, 0.2, 0.3, 0.4],[0.1, 0.2, 0.3, 0.4]]
    # print(process_coords(coords))
    # print(iou([0.3, 0.3, 0.5, 0.5], [0.4, 0.4, 0.6, 0.6]))
    # input_image_path = "/home/kevin/androidworld2/SAM/augment/1/original.jpg"
    
    # # 归一化后的边界框坐标 (左上角和右下角)
    # normalized_bbox_coordinates = (0.05, 0.15, 0.95, 0.94)  # 坐标范围在 [0, 1]
    
    # # 输出图片路径
    # output_image_path = "/home/kevin/androidworld2/SAM/augment/1/original_with_box.jpg"
    
    # # 调用函数绘制边界框
    # draw_bbox_on_image(input_image_path, normalized_bbox_coordinates, output_image_path)
#     image_path = "/home/kevin/androidworld2/SAM/images/dog1.JPG"
#     point_coords = [[0.5, 0.5]]  # 示例前景点
#     point_labels = [1]           # 正样本
#     bbox = [0.25, 0.1, 0.75, 0.9]   # 示例 BBox

#     best_mask, overlayed_image = run_sam_segmentation(
#     image_path=image_path,
#     point_coords=point_coords,
#     point_labels=point_labels,
#     bbox=bbox,
#     model_type="vit_b",
#     checkpoint_path="sam_check_point/sam_vit_b_01ec64.pth"
# )
    # draw_bboxes_on_image(image_path= '/home/kevin/androidworld2/SAM/images/dog1.JPG', bboxes= [[0.1,0.5,0.98,0.8], [0.4,0.4,0.6,0.6]])
    text = """Best Box: 4 (blue)
Reasoning: Bbox 3 encloses the entire dog most accurately, considering both width and height. The dog is completely within the dimensions of Bbox 3 without being cut off, allowing some margin around the edges for movement or postural changes. Bbox 1 and Bbox 4 may include the dog but Bbox 3 offers a larger margin ensuring the full body is captured in most scenarios. Bbox 2 is too small and misses parts of the dog."""
    print(extract_best_box_number(text))
